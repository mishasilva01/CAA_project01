{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa42fe9",
   "metadata": {},
   "source": [
    "# 1. First steps\n",
    "\n",
    "______________\n",
    "\n",
    "Importing Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from tensorflow.keras.metrics import Precision\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Literal, Dict\n",
    "from itertools import combinations, product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66768e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "df_train = pd.read_csv(\"set_train@val.csv\")\n",
    "df_test = pd.read_csv(\"set_test.csv\") \n",
    "\n",
    "# drop first column (index)\n",
    "df_train = df_train.iloc[:,1:]\n",
    "df_test = df_test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087521a",
   "metadata": {},
   "source": [
    "# 2. Getting acquainted with data\n",
    "\n",
    "_________________\n",
    "\n",
    "- Categorical features:\n",
    "   - *age*\n",
    "   - *trestbps* - resting blood pressure /*mm Hg* (on admission to the hospital)\n",
    "   - *chol* - serum cholesterol in *mg/dl*\n",
    "   - *thalach* - Maximum heart rate achieved /*bps*\n",
    "   - *oldpeak* -  ST depression induced by exercise relative to rest \n",
    "   - *ca* - Number of major vessels (0-3) colored by fluoroscopy\n",
    "\n",
    "- Integer features:\n",
    "   - *sex* \n",
    "       - 0: female\n",
    "       - 1: male\n",
    "   - *cp* - chest pain\n",
    "       - 1: typical angina\n",
    "       - 2: atypical angina\n",
    "       - 3: non-anginal pain\n",
    "       - 4: asymptomatic\n",
    "   - *fbs* - fasting blood sugar > 120 mg/dl\n",
    "       - 0: False\n",
    "       - 1: True\n",
    "   - *restecg* - Resting electrocardiographic results\n",
    "       - 0: Normal\n",
    "       - 1: Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "       - 2: Showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "   - *exang* - Exercise induced angina\n",
    "       - 0: No\n",
    "       - 1: Yes\n",
    "   - *slope* - The slope of the peak exercise ST segment\n",
    "       - 1: Up sloping\n",
    "       - 2: Flat\n",
    "       - 3: Down sloping\n",
    "   - *thal*\n",
    "       - 3: Normal\n",
    "       - 6: Fixed defect\n",
    "       - 7: Reversible defect\n",
    "- Label\n",
    "    - *num* - The final diagnosis of heart disease (angiographic disease status)\n",
    "\n",
    "##### The information was taken from:\n",
    "Nassif A. (et al). 2018. Machine Learning Classifications of Coronary Artery Disease. DOI: 10.1109/iSAI-NLP.2018.8692942"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c21c0",
   "metadata": {},
   "source": [
    "### 2.1 Checking training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b46f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee3d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e3f6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking the number of missing values in each feature\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311225c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of samples per class\n",
    "df_train.num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = int(np.ceil(len(df_train.columns) / (1.0*nrows)))\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(30, 15))\n",
    "counter = 0\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "\n",
    "        ax = axes[i][j]\n",
    "        if counter < len(df_train.columns):\n",
    "\n",
    "            ax.hist(df_train[df_train.columns[counter]], bins=10, color='blue', alpha=0.5)\n",
    "            ax.set_ylabel(df_train.columns[counter])\n",
    "\n",
    "        else: ax.set_axis_off()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085a33c",
   "metadata": {},
   "source": [
    "### 2.2 Checking test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff244af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05939cb3",
   "metadata": {},
   "source": [
    "# 4. Feature Selection\n",
    "\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Features selection by correlation elimination\n",
    "\n",
    "___________\n",
    "\n",
    "We are using Spearman's correlation instead of Pearson's because practically all features don't have a normal distribution (as seen in the histograms above) and Spearman's is more robust to outliers.\n",
    "\n",
    "Nassif et al. (2018; 10.1109/iSAI-NLP.2018.8692942) also applied correlation evaluation, but did not use Spearman's correlation.\n",
    "\n",
    "Creating a function that selects features according correlation. The arguments of the function (1) the dataframe, (2) the correlation method, and (3) the correlation threshold for feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corr_selection(df: pd.DataFrame, corr_method: str = 'spearman', threshold: float = 0.2) -> List:\n",
    "\n",
    "    \"\"\"This function selects the features to be eliminate according to a correlation threshold.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe - Dataset containing all data.\n",
    "        corr_method: str - Which method to apply to calculate the correlation. Default is 'spearman'.\n",
    "        threshold: float - a limit between 0 and 1. Default is 0.2.\n",
    "    Return:\n",
    "        a list value, containing the selected features.\n",
    "        \n",
    "    \"\"\"\n",
    "    corr_matrix = X.corr(method=corr_method)\n",
    "    # covert corr_matrix to absolute values\n",
    "    corr_matrix = np.array(abs(corr_matrix))\n",
    "    # all features available\n",
    "    features = np.array(df.columns)\n",
    "    # list of features to eliminate\n",
    "    to_eliminate = []\n",
    "    # runs all lines\n",
    "    for i in range(len(features)):\n",
    "        # runs all columns\n",
    "        for j in range(len(features)):\n",
    "            \n",
    "            if corr_matrix[i,j] > threshold and corr_matrix[i,j] != 1: \n",
    "                \n",
    "                if np.mean(corr_matrix[i,:]) > np.mean(corr_matrix[:,j]): to_eliminate.append(features[i])\n",
    "                else: to_eliminate.append(features[j])\n",
    "    \n",
    "    to_eliminate = np.unique(to_eliminate)\n",
    "    selected_features = list( features[~np.isin(features, to_eliminate)] )\n",
    "    return [selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Features selection by information gain\n",
    "\n",
    "__________\n",
    "\n",
    "Creating a function that selects features according to Mutual Information Gain. \n",
    "\n",
    "References:\n",
    "- Nassif et al. (2018; 10.1109/iSAI-NLP.2018.8692942)\n",
    "- Guhanesvar. *Feature Selection Based on Mutual Information Gain for Classification and Regression*. Medium. June 26, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain_selection(X: pd.DataFrame, y: pd.DataFrame, n_features: int) -> Tuple[pd.Series, List]:\n",
    "\n",
    "    \"\"\"This function selects n_features according that have the greatest mutual information classification.\n",
    "\n",
    "    Args:\n",
    "        X: Dataframe - Dataset containing all data.\n",
    "        y: Dataframe - Dataset containing all target data.\n",
    "        n_features: int - Number of features to select.\n",
    "    Return:\n",
    "        a tuple of a Series and a list, of the importance and the selected features.\n",
    "    \"\"\"\n",
    "    \n",
    "    importance = mutual_info_classif(X, y)\n",
    "    importance = pd.Series(data = importance, index = X.columns)\n",
    "    importance = importance.sort_values(ascending=False)\n",
    "    selected_features = list(importance.iloc[:n_features].index)\n",
    "    return importance, [selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecc673",
   "metadata": {},
   "source": [
    "# 5. Class imbalance\n",
    "\n",
    "_____________\n",
    "\n",
    "As seen in the histogram of Fig. 14, there are many more samples with label 0 than with other labels. Therefore, class imbalance has to be dealt with. According to Hoffman [1], there are several ways to reduce class imbalance, such as using other performance metrics when assessing performance (precision, recall, F1 score, and confusion matrix), performing over-sampling (when the dataset is reletively small), or using the Synthetic Minority Over-sampling Technique (SMOTE) algorithm to create synthetic samples.\n",
    "\n",
    "BUT, according to this video https://www.youtube.com/watch?v=adHqzek--d0, on stamp 3:52, the SMOTE technique is not good for multidimensional data (which is the one we have here)\n",
    "\n",
    "REFERENCE [1]: K. Hoffman. *Machine Learning: How to Handle Class Imbalance*. Analytics Vidhya. February 13, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4428e",
   "metadata": {},
   "source": [
    "### 5.1 Oversampling by doubling classes different from 0\n",
    "\n",
    "_____________\n",
    "\n",
    "This method might create oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_double(dataframe: pd.DataFrame) -> pd.DataFrame:   \n",
    "    \n",
    "    \"\"\"This function doubles the number of samples with labels 1, 2, 3, 4.\n",
    "\n",
    "    Args:\n",
    "        dataframe: DataFrame - Dataset containing all data.\n",
    "    Return:\n",
    "        a DataFrame value.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in [1, 2, 3, 4]:\n",
    "\n",
    "        df_add = dataframe[dataframe['num']==i]\n",
    "        dataframe = pd.concat([dataframe, df_add], axis=0)\n",
    "\n",
    "    # Restarting indices\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76f129",
   "metadata": {},
   "source": [
    "### 5.2 Oversampling through SMOTE\n",
    "\n",
    "____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_smote(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" This function oversamples a dataset with the Synthetic Minority Over-sampling Technique (it is required imblearn).\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame - Dataset containing all data.\n",
    "    Return:\n",
    "        a DataFrame value.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    smote = SMOTE(random_state = 42)\n",
    "    X, y = smote.fit_resample(dataframe.drop(columns='num'), dataframe['num'])\n",
    "    y = pd.Series(data = y, name = 'num')\n",
    "    df_smote = pd.concat([X, y], axis = 1, join = 'inner')\n",
    "    return df_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Oversampling through Class Weight\n",
    "\n",
    "____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassWeight(dataframe: pd.DataFrame) -> Dict:\n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    classes = dataframe.iloc[:,-1].unique()\n",
    "    weights = class_weight.compute_class_weight(class_weight = 'balanced', \n",
    "                                                classes = classes, \n",
    "                                                y = dataframe.iloc[:,-1])\n",
    "    ClassWeight = dict()\n",
    "    for i in range(len(classes)): ClassWeight[classes[i]] = weights[i]\n",
    "    return ClassWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating Models\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724a6d5",
   "metadata": {},
   "source": [
    "### 6.1 Convolutional Neural Networks\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_train.drop('num', axis=1))\n",
    "y = np.array(df_train['num'])\n",
    "X_test = np.array(df_test.drop('num', axis=1))\n",
    "y_test = np.array(df_test['num'])\n",
    "\n",
    "# one-hot encoding\n",
    "y_onehot = OneHotEncoder(sparse=False).fit_transform(y.reshape(-1, 1))\n",
    "y_test_onehot = OneHotEncoder(sparse=False).fit_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# split dataset for training validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1d(X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "            n_conv_layers=1, filter=[64], kernel_size=[3], conv_activation='relu', \n",
    "            ann_activation='softmax'):\n",
    "\n",
    "    # normalization\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_val_norm = scaler.transform(X_val)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    # reshaping data to fit into CNN model\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filter[0], kernel_size=kernel_size[0], activation=conv_activation, input_shape=(13, 1)))\n",
    "    # adding convolutional layers\n",
    "    for i in range(1, n_conv_layers):\n",
    "        model.add(Conv1D(filters=filter[i], kernel_size=kernel_size[i], activation=conv_activation))\n",
    "        # input_shape = model.layers[-1].output_shape[1:]\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    # last ANN layer\n",
    "    length_flat = np.prod(model.output_shape[1:])\n",
    "    model.add(Dense(round(length_flat/2), activation=ann_activation))\n",
    "    # output layer\n",
    "    model.add(Dense(5, activation=ann_activation))\n",
    "\n",
    "    # compiling the model\n",
    "    # we use Precision as the training metric because it is the best for heart disease detection\n",
    "    # and it focuses on evaluating the true positives rate \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[Precision()])\n",
    "\n",
    "    # fitting the model\n",
    "    model.fit(X_train_norm, y_train, epochs=500, batch_size=32, validation_data=(X_val_norm, y_val))\n",
    "    history = model.history.history\n",
    "\n",
    "    # prediction\n",
    "    prediction = model.predict(X_test_norm)\n",
    "\n",
    "    # undoing one-hot encoding\n",
    "    prediction_decoded = [np.argmax(x) for x in prediction]\n",
    "    y_test_decoded = [np.argmax(x) for x in y_test]\n",
    "\n",
    "    # evalutation\n",
    "    accuracy = accuracy_score(y_test_decoded, prediction_decoded)\n",
    "    precision = precision_score(y_test_decoded, prediction_decoded, average='macro')\n",
    "    recall = recall_score(y_test_decoded, prediction_decoded, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test_decoded, prediction_decoded)\n",
    "\n",
    "    return accuracy, precision, recall, conf_matrix, history, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91510a",
   "metadata": {},
   "source": [
    "- The candidate parameters are mainly the number of layers, number of filters and size of filters.\n",
    "- <b>NOTE:</b> When using 32 filters, we use filters of size 3 (less deep, but wider), 64 filters with size 4, and 128 filters with size 5 (deeper but norrower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate parameters \n",
    "n_conv_layers = [1, 2, 3]\n",
    "all_filters = [\n",
    "    [[32], [64], [128]], \n",
    "    [[32, 64], [64, 128], [32, 128]], \n",
    "    [[32, 64, 128], [64, 128, 32]]\n",
    "    ]\n",
    "all_kernels = [\n",
    "    [[3], [4], [5]], \n",
    "    [[3, 4], [4, 5], [3, 5]], \n",
    "    [[3, 4, 5], [4, 5, 3]]\n",
    "    ]\n",
    "conv_activation = ['sigmoid', 'relu', 'softmax']\n",
    "ann_activation = ['sigmoid', 'relu', 'softmax']\n",
    "\n",
    "# creating models\n",
    "cnn1d_results = {\"n_conv_layers\": [], \"n_filters\": [], \"kernel_sizes\": [], \"conv_activation\": [],\n",
    "                 \"ann_activation\": [], \"accuracy\": [], \"precision\": [], \"recall\": [],\n",
    "                 \"conf_matrix\": [], \"history\": [], \"prediction\": []}\n",
    "\n",
    "for n_conv in n_conv_layers:\n",
    "    filters = all_filters[n_conv]\n",
    "    kernels = all_kernels[n_conv]\n",
    "    \n",
    "    for k in range(len(filters)):\n",
    "        filt_layer = filters[k]\n",
    "        kern_layer = kernels[k]\n",
    "        \n",
    "        for conv_act in conv_activation:\n",
    "            for ann_act in ann_activation:\n",
    "\n",
    "                accuracy, precision, recall, conf_matrix, history, prediction = conv_1d(\n",
    "                    X_train, X_val, X_test, y_train, y_val, y_test, n_conv_layers=n_conv, \n",
    "                    filter=filt_layer, kernel_size=kern_layer, conv_activation=conv_act, \n",
    "                    ann_activation=ann_act)\n",
    "                \n",
    "                cnn1d_results[\"n_conv_layers\"].append(n_conv)\n",
    "                cnn1d_results[\"n_filters\"].append(filt_layer)\n",
    "                cnn1d_results[\"kernel_sizes\"].append(kern_layer)\n",
    "                cnn1d_results[\"conv_activation\"].append(conv_act)\n",
    "                cnn1d_results[\"ann_activation\"].append(ann_act)\n",
    "                cnn1d_results[\"accuracy\"].append(accuracy)\n",
    "                cnn1d_results[\"precision\"].append(precision)\n",
    "                cnn1d_results[\"recall\"].append(recall)\n",
    "                cnn1d_results[\"conf_matrix\"].append(conf_matrix)\n",
    "                cnn1d_results[\"history\"].append(history)\n",
    "                cnn1d_results[\"prediction\"].append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Models Evaluation\n",
    "\n",
    "_______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 1D-CNN\n",
    "\n",
    "______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1d_results_df = pd.DataFrame(cnn1d_results)\n",
    "cnn1d_results_df_precison = cnn1d_results_df.sort_values(\"precision\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1.1 Confusion Matrix (best precision)\n",
    "________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a568060",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(cnn1d_results_df_precison['conf_matrix'], cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cnn1d_results_df_precison['conf_matrix'].shape[0]):\n",
    "    for j in range(cnn1d_results_df_precison['conf_matrix'].shape[1]):\n",
    "        ax.text(x = j, y = i, s = cnn1d_results_df_precison['conf_matrix'][i, j], \n",
    "                va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix of best model (precision)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1.2 Loss function (best precision)\n",
    "\n",
    "________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_training_loss = cnn1d_results_df_precison.history.loc[0][\"loss\"]\n",
    "cnn_validation_loss = cnn1d_results_df_precison.history.loc[0][\"val_loss\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cnn_training_loss)\n",
    "plt.plot(cnn_validation_loss)\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Standard DNN\n",
    "\n",
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1 Confusion Matrix\n",
    "\n",
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2 Precision\n",
    "\n",
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd4e59",
   "metadata": {},
   "source": [
    "# 8. References\n",
    "<a id=\"1\">[1]</a>\n",
    "Anggoro, Dimas Aryo and Kurnia, Naqshauliza Devi (2020). \n",
    "Comparison of accuracy level of support vector machine (SVM) and K-nearest neighbors (KNN) algorithms in predicting heart disease\n",
    "International Journal, 8(5), 1689--1694.\n",
    "\n",
    "<a id=\"2\">[2]</a>\n",
    "https://www.kaggle.com/code/zawlinnnaing/svm-for-multiclass-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
